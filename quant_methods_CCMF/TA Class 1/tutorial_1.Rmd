---
title: "tutorial_1"
author: "Lir Hoxhaj"
date: "2025-01-09"
output: html_document
---

# Background

**GOALS OF TUTORIAL**: 
1. Run a single and a multiple variable regression using OLS method
2. Be able to interpret results and understand key indicators in regression table
3. Understand RCT and how we can analyse the impact of an experiment by analysing random groups using statistical methods
4. Use libraries to process data, change shape of datasets by using R libraries


**Sources**:
- Cognitive data: https://github.com/ryanfalck/R_Help
- Smoker data: https://www.kaggle.com/datasets/harrywang/propensity-score-matching?resource=download&select=smoker.csv
- Spotify data: https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset?select=dataset.csv
- Climate data: https://www.kaggle.com/datasets/tarunrm09/climate-change-indicators

# Setup

Let's set up a new environment and load our libraries in one place.

```{r}
rm( list = ls(all.names = TRUE) )

## Installing libraries

#install.packages("readxl")
#install.packages("rdrobust")
if(!require(readxl)) install.packages("readxl")
if(!require(rdrobust)) install.packages("rdrobust")
if(!require(AER)) install.packages("AER",repos = "http://cran.us.r-project.org")
if(!require(tidyr)) install.packages("tidyr")
if(!require(scatterplot3d)) install.packages("scatterplot3d")
if(!require(tableone)) install.packages("tableone")
if(!require(lsmeans)) install.packages("lsmeans")

## Loading libraries
library(rstudioapi)
library(readxl)
library(dplyr)
library(ggplot2)
library(rdrobust)
library(AER)
library(corrplot)
library(scatterplot3d)
library(tableone)
library(lsmeans)
```

Setting up our working directory

```{r}
# Automatically (based on location of this script)
working_directory <- dirname(rstudioapi::getActiveDocumentContext()$path)

# , or manually...
# setwd("C:\\Users\\lhoxhaj\\OneDrive - Imperial College London\\...\\folder_name") # set working directory
# rct_data <- read_excel("rct_data.xlsx") # read data

```



# 1. Regressions

## Motivation

For any statistical method (from simple linear regressions to neural networks), our goal is to try and approximate $f(X)$, which is the 'rule' or function that describes the relationship between our independent variables $X$ and dependent variable $Y$. This is what a statistical learning / machine learning model does. They can help us *infer* the relationship between independent variables(s) $X$ and dependent variable/outcome $Y$, as well as *predict* future values of an outcome $Y$ given values of $X$.

$$\Large Y = f(X) + e$$
In reality, there is one true $f(X)$ but we don't know this, because we don't have the population data to determine this and we cannot precisely account for every factor (i.e, every $X$) in the universe that affects $Y$. Goal of statistics is to figure out $f$, the true relationship between the two. We approximate this relationship $\hat{f(X)}$ using sample data and several assumptions about the functional form. Using our prediction function, we get predicted values of the outcome:

$$\Large \hat{Y} = \hat{f(X)}$$

The accuracy of our function can be measured as the average/expected difference between the true outcome values we have $(Y)$ and the predicted outcomes of our model given values of $X$ $(\hat{Y})$. Our goal here is to minimise the error (difference between the two):

$$\Large E[Y-\hat{Y}] = E[Y-\hat{f(X)}]$$


## Simple Linear Regression

In conducting linear regression, we are hypothesizing that the relationship between our dependent and independent variables is linear. This can be expressed using the simple linear regression model:

$$\Large Y_i = \beta_0 + \beta_1X_i + u_i$$
where:
-   index $i$ covers all the observations - $i = 1,...,n$
-   $Y_i$ is the **dependent** variable, also known as **regressand** or **left-side**
-   $X_i$ is the **independent** variable, also known as **regressor** or **right-side**
-   $Y = \beta_0 + \beta_1X$ is the **regression line**
-   $\beta_0$ is the regression line **intercept**
-   $\beta_1$ is the regression line **slope** (sometimes **coefficient**)
-   $u_i$ is the **error term**

In practice, $\beta_0$ and $\beta_1$ are unknown, so it is an objective of regression analysis to estimate these parameters. To demonstrate, we will use the real-world example of test scores and student-teacher ratios detailled previously.

$$\Large \hat{Y_i} = \hat{\beta_0} + \hat{\beta_1}X_i$$

, where the error is the difference between predicted outcome and the true outcome.

Our objective will be to discover the relationship between test scores and student-teacher ratios. We can retrieve data for this experiment from the AER package, installed from CRAN.

From this library, we can load the data set with the function data():

```{r}
data(CASchools)
```

The CASchools data is made up of California state-wide maths and reading scores for fifth grade students. Class size ratio is measured as students divided by teachers.

We can see that we have a mix of numeric and categorical variables in the data. More importantly, the variables making up our investigation - test score and student-teacher ratio - are not included and will have to be calculated.

### Data processing 

For student-teacher ratio, we divide the number of students by number of teachers:

```{r}
CASchools$st_ratio <- CASchools$students / CASchools$teachers
```

For test scores, we calculate the arithmetic mean of the reading and maths test scores:

```{r}
CASchools$test_score <- (CASchools$read + CASchools$math)/2
```

Summarising dataset

```{r}
# print summary to console
summary(CASchools)
```

We now have more insight into the shape of our data, allowing us to identify means, standard deviations and potential outliers.

To make outliers even easier to detect, let's plot the data on a scatterplot with some helpful annotations:

```{r}
ggplot(CASchools, aes(x = st_ratio, y = test_score)) +
  geom_point(color = "blue", alpha = 0.7) +  # Scatter points
  labs(
    title = "Scatterplot of test scores and student-teacher ratio",  # Title
    x = "student-teacher ratio (x)",  # X-axis label
    y = "test score (y)"  # Y-axis label
  ) +
  theme_minimal()  # Clean minimal theme
```

Correlation is any statistical relationship, whether causal or not, between two random variables or bivariate data. 

$$\Large cor(x,y) = \frac{cov(x,y)}{\sqrt{var(x) \cdot var(y)}}$$

We can see a strong scatter pattern in the data with an apparent negative correlation (we expect to see lower test scores in larger classes). 
Correlation can be tested by calling cor():

```{r}
cor(CASchools$st_ratio, CASchools$test_score)
```

A weak negative correlation confirms our observation from the scatterplot. We could now simply eyeball a best-fit line onto the plot based on the correlation, however this would be subjective to each observer of the data (research must be reproducible for it to be reliable!)

### The Ordinary Least Squares (OLS) Estimator

The OLS estimator selects regression coefficients $\beta_0$ and $\beta_1$ such that the regression line will be as "close" to as many data points as possible. 'Closeness' in this case is measured by the sum of squared errors made in predicting $Y$ given $X$. In other terms, we want to draw (in econometrics, we say fit a line) that is will have a minimal distance between itself and all observations. This 'distance' is squared because of positive and negative distance from the line to the observations. 

If we take $b_0$ and $b_1$ to be some estimators of $\beta_0$ and $\beta_1$, the sum of squared errors can be expressed as: 

$$\Large \min_{\hat{\beta_0}, \hat{\beta_1}} SSR = \min_{\hat{\beta_0}, \hat{\beta_1}} \sum_{i=1}^n u_i^2 = \min_{\hat{\beta_0}, \hat{\beta_1}} \sum_{i=1}^n (Y_i - \hat{Y}i)^2 = \min_{\hat{\beta_0}, \hat{\beta_1}} \sum_{i=1}^n (Y_i - \hat{\beta_0} - \hat{\beta_1} X_i)^2 $$


The OLS estimator is simply the pair of estimators that minimizes this expression. After finding a minimum with respect to both of these coefficients, we get:

$$
\Large
\begin{aligned}
\hat{\beta}_1 &= \frac{\sum^n_{i=1}(X_i - \bar{X})(Y_i - \bar{Y})}{\sum^n_{i=1}(X_i - \bar{X})^2}\\
\\
\hat{\beta}_0 &= \bar{Y} - \hat{\beta}_1\bar{X}
\end{aligned}
$$

$\bar{X}$ and $\bar{Y}$ indicate the mean of the $X$ and $Y$ variable respectively.

Note for $\hat{\beta_1}$, after diving by $N-1$, we have covariance of $X$ and $Y$ (how much X and Y vary together on average from their respective means) and variance of $X$ (captures variability of X).

$$
\Large
\begin{aligned}
\hat{\beta}_1 &= \frac{\sum^n_{i=1}(X_i - \bar{X})(Y_i - \bar{Y})}{\sum^n_{i=1}(X_i - \bar{X})^2} = \frac{\frac{\sum^n_{i=1}(X_i - \bar{X})(Y_i - \bar{Y})}{N-1}}{\frac{\sum^n_{i=1}(X_i - \bar{X})^2}{N-1}} = \frac{cov(X,Y)}{var(X)}
\end{aligned}
$$

### Calculating OLS Estimators in R

R contains a handy built-in function for performing regression analysis - lm() (linear model). We can call lm() on our data set to obtain an intercept and slope estimate:

```{r}
# estimate and assign result
lin_model <- lm(test_score ~ st_ratio, data = CASchools)
#print result
summary(lin_model)
```

$$
\begin{aligned}
\Large \hat{\beta_0} = 698.93 \\
\Large \hat{\beta_1} = -2.2798 \\
\end{aligned}
$$

$\hat{\beta_1}$ interpretation: for an increase of one unit in student-teacher ratio (i.e., one student per teacher), test scores, on average all else held constant, will decrease by -2.2798 units.

Let's add these estimates to our earlier plot. We also need to extend the axis ranges with arguments xlim and ylim

```{r}
ggplot(CASchools, aes(x = st_ratio, y = test_score)) +
  geom_point(color = "blue", alpha = 0.7) +
  geom_smooth(method = "lm", color = "red", se = TRUE) +  # Add regression line
  labs(
    title = "Scatterplot of test scores and student-teacher ratio",
    x = "student-teacher ratio (x)",
    y = "test score (y)"
  ) +
  theme_minimal()

```


### Hypothesis Testing in the Simple Linear Regression Model

Now we have our regression results, the next task is to test our hypothesis that test scores and teacher-student ratio hold a linear relationship. It is not enough to run a regression and report the intercept and slope! Although they can also be stated as hypothetical ranges of values a variable may take, we state hypothesis when understanding regression coefficients as a binary "yes or no" question that is either accepted or rejected.

Formally, we begin by stating our hypothesis regarding the relationship:

$$
\begin{aligned}
\Large H_0 : \beta_1 = 0 \\
\Large H_1 : \beta_1 \neq 0
\end{aligned}
$$

$H_0$ forms our null hypothesis, the hypothesis we want to test. $H_1$ is our alternate hypothesis, the hypothesis that is held if the null hypothesis is rejected.

In this scenario, our null hypothesis is that $\beta_1$ (slope) is equal to zero, with the alternate being that $\beta_1$ is not equal to zero. This is a two-sided hypothesis, as $\beta_1$ can take values on either side of the probability distribution (which for we assume to be approximately normal for $\hat{\beta_1}$).

To conduct our test, we need to calculate:

1.  The standard error of $\hat{\beta_1}$ ($SE\hat{\beta_1}$). This is a measure of how much our approximation of the slope of the regression line varies.

$$
\Large
SE(\hat{\beta_1}) = \sqrt{\hat{\sigma}^2_{\hat{\beta}_1}}
$$

2.  The $t$-statistic, a measure of how much away is the estimated slope coefficient value from the hypothesised one (in our case, $H_0: \beta_1 = 0$, based on the null hypothesis). T-statistic is used for t-distributions, which we use when we have sample data.

$$
\Large
\begin{aligned}
t &= \frac{\hat{\beta_1}-\beta_{1,0}}{SE(\hat{\beta_1})}\\
\\
t &= \frac{\text{estimated value} - \text{hypothesized value}}{\text{standard error of estimator}}
\end{aligned}
$$

Using the results from our regression summary, we have the t-statistic:

$$ \Large t = \frac{-2.2798 - 0}{0.4798} \approx -4.75 $$

3.  The $p$-value, given that we have a two-sided $H_1$:

$$
\Large
p-value = Pr_{H_0} \left[\left|\frac{\hat{\beta}_1-\beta_{1,0}}{SE(\hat{\beta}_1)}\right|>\left|\frac{\hat{\beta}^act_1-\beta_{1,0}}{SE(\hat{\beta}_1)}\right|\right]
$$


This notation looks intimidating, but R calculates all this for you! You just need to know what to look for in the output!

p-value is a number describing how likely it is that something would have occurred by random chance (i.e., that the null hypothesis is true).

From the output, we can state our regression model with the numbers plugged in:

Because we have a two-sided hypothesis, we can reject our $H_0$ at $5\%$ significance using two methods:

1.  Critical value rejection - our critical value $-4.75$ falls within the rejection region $|Z| < -1.96$
2.  $p$-value rejection - our $p$-value is significantly less than $0.05$ ($2.78\cdot10^{-6} < 0.05$).

From this, we can conclude that our $\beta_1$ coefficient is significantly different from zero. Subsequently, we can reject the null hypothesis that class size ($st\_ratio$) has no influence on student's scores ($test\_scores$) at the $5\%$ level of significance.

Visually, we can demonstrate this with a plot of the $t$ distribution:

```{r, echo=F}
# Plot the standard normal on the support [-6,6]
t <- seq(-6, 6, 0.01)

plot(x = t, 
     y = dnorm(t, 0, 1), 
     type = "l", 
     col = "steelblue", 
     lwd = 2, 
     yaxs = "i", 
     axes = F, 
     ylab = "", 
     main = expression("Calculating the p-value of a Two-sided Test when" ~ t^hat ~ "=-4.75"), 
     cex.lab = 0.7,
     cex.main = 1)

tact <- -4.75

axis(1, at = c(0, -1.96, 1.96, -tact, tact), cex.axis = 0.7)

# Shade the critical regions using polygon():

# critical region in left tail
polygon(x = c(-6, seq(-6, -1.96, 0.01), -1.96),
        y = c(0, dnorm(seq(-6, -1.96, 0.01)), 0), 
        col = 'orange')

# critical region in right tail

polygon(x = c(1.96, seq(1.96, 6, 0.01), 6),
        y = c(0, dnorm(seq(1.96, 6, 0.01)), 0), 
        col = 'orange')

# Add arrows and texts indicating critical regions and the p-value
arrows(-3.5, 0.2, -2.5, 0.02, length = 0.1)
arrows(3.5, 0.2, 2.5, 0.02, length = 0.1)

arrows(-5, 0.16, -4.75, 0, length = 0.1)
arrows(5, 0.16, 4.75, 0, length = 0.1)

text(-3.5, 0.22, 
     labels = expression("0.025"~"="~over(alpha, 2)),
     cex = 0.7)
text(3.5, 0.22, 
     labels = expression("0.025"~"="~over(alpha, 2)),
     cex = 0.7)

text(-5, 0.18, 
     labels = expression(paste("-|",t[act],"|")), 
     cex = 0.7)
text(5, 0.18, 
     labels = expression(paste("|",t[act],"|")), 
     cex = 0.7)

# Add ticks indicating critical values at the 0.05-level, t^act and -t^act 
rug(c(-1.96, 1.96), ticksize  = 0.145, lwd = 2, col = "darkred")
rug(c(-tact, tact), ticksize  = -0.0451, lwd = 2, col = "darkgreen")
```


The rejection region is highlighted in yellow, being where our $t$-statistic resides.


### Measures of Fit

As with any estimation, we want to know how accurate we are! More specifically, we want to know how well our model fits the data. This can be assessed visually by the degree of clustering the data points have around the regression line. Numerically, we can call on the coefficient of determination - known as $R^2$. $R^2$ is the fraction of the sample variance of $Y_i$ that is explained by $X_i$. Mathematically, this is the ratio of the explained sum of squares ($ESS$) to the total sum of squares ($TSS$).

$$ \Large R^2 = \frac{ESS}{TSS}$$

Since $TSS$ = $ESS$ + $SSR$ (or $RSS$), we have:

$$ \Large R^2 = \frac{TSS-RSS}{TSS} = 1-\frac{RSS}{TSS} $$

The $ESS$ is the sum of squared deviations from the predicted values $\hat{Y}_i$, from their average $\bar{Y_i}$:

$$
\Large
ESS = \sum^n_{i-1}(\hat{Y}_i - \bar{Y})^2
$$

The $TSS$ is the sum of squared deviations from $Y_i$ from their average $\bar{Y}_i$

$$
\Large
TSS = \sum^n_{i-1}(Y_i - \bar{Y})^2
$$

$SSR$ is a measure of errors made when predicting $Y$ by $X$, defined as:

$$
\Large
SSR = \sum^n_{i=1}\hat{u}^2_i
$$

The resulting $R^2$ figure lies between 0 and 1, where 1 indicates perfect fit (no errors made) $(ESS=TSS)$ and 0 $(ESS=0)$ indicates no fit.


```{r}
summary(lin_model)
```

Our $R^2$ is $0.051$, so $5.1\%$ of the variance in dependent variable test_score is explained by explanatory variable st_ratio. Additionally, the $SER$ is $18.58$, meaning that the average deviation between the actual test score and the regression line is $18.58$ points.



## The Multiple Linear Regression Model

Simple linear regression analysis is fine for illustrating the fundamentals of analysing a relationship between variables, but it is majorly flawed in that it does not consider other determinants of the dependent variable. In other words, there are other explanatory elements in the relationship that we are not identifying with our model.

For an OLS regression to be *unbiased*, we make many assumptions, one of them being that the independent variables are linearly independent ([more info here](https://en.wikipedia.org/wiki/Linear_regression#Assumptions)). However, we may violate this assumption and induce bias if we do not include determinants of the dependent variable that vary with the independent variable (recall that influences on $test\_score$ that are not captured are collected as the error term $u$).

Ultimately, if we do not capture these additional determinants, we will wrongly estimate the causal effect on $test\_score$ of a unit change in $st\_ratio$ - known as **omitted variable bias (OVB)**

In the CASchools data set we have many other variables that may cause bias if they are omitted from the model. A potentially relevant omitted variable could be the percentage of English learners in the school district. It stands to reason that the ability to speak, read, and write English is an important factor in successful learning. Additionally, it is possible that the portion of English learner students is bigger in school districts where class sizes are relatively large - i.e. in poor urban districts with higher population of immigrants. Therefore, English learner students are likely to perform worse in assessments than native speakers.

We can demonstrate the difference including percentage of English learners has on the model by computing correlations between $st\_ratio$, $test\_score$, and $english$:

```{r}
cor(CASchools$st_ratio, CASchools$test_score)
cor(CASchools$st_ratio, CASchools$english)
```

We are already familiar with the negative correlation between $st\_ratio$ and $test\_score$, where smaller class sizes appear to positively influence test scores (or, as class sizes grow, scores appear to decrease). However, the sign of the correlation between $st\_ratio$ and $english$ is positive - meaning that as class sizes grow, percentage of English learners increases.

Where this becomes problematic in regression analysis is how the estimators become biased. Presently, our estimator $\hat{\beta_1}$ is suggesting that smaller class sizes improve test scores. But $\hat{\beta_1}$ is also overestimating the effect of small classes because it is not capturing the effects of having fewer English learners. Therefore, we need to update our regression model to include $english$ as an independent variable:


The multiple regression model with $p$ number of predictors will look like this:

$$
\Large Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \beta_3X_{3i} + \cdot \cdot \cdot + \beta_kX_{ki} + u_i, i = 1,...,n \\
\Large Y_i = \beta_0 + \sum_{j=1}^p \beta_jX_{ji} \\
$$

or in the context of our data:


### Estimating Multivariate Regression

Let's estimate both models and see the difference:

```{r}
# estimate models
linear_mod <- lm(test_score ~ st_ratio, data = CASchools)
multi_mod <- lm(test_score ~ st_ratio + english, data = CASchools)
```

```{r}
# print results
summary(linear_mod)
summary(multi_mod)
```

The multiple regression now captures the effects of the independent variable $english$. We still have a negative (but larger) estimate for $\hat{\beta}_1$ and a negative estimate for $\hat{\beta}_2$. This is an indication of great OVB. Note that there are instance where the significance and even the direction of the impact (coefficient sign) may change entirely because of this newly introduced variance.

Multiple regression models may contain many independent variables, and we need to reliably estimate the effects on dependent variable $Y_i$ of changing independent variable $X_{1i}$ if remaining right-side variables $X_{2i}, X_{3i},...,X_{ki}$ do not change (*ceteris paribus*).

In our model, coefficient scores on $st\_ratio$ are interpreted as the effect on $test\_score$ of a **one unit** change in $st\_ratio$ if $english$ is held constant.

Similar to the simple regression model, we assume that the true relationship between dependent and independent variables is linear with an error term $u$ that we cannot observe.

```{r, echo=F, message=F}
require("scatterplot3d")

# Data, linear regression with two explanatory variables
#wh <- iris$Species != "setosa"
x  <- CASchools$st_ratio#[wh]
y  <- CASchools$test_score#[wh]
z  <- CASchools$english#[wh]
df <- data.frame(x, y, z)
LM <- lm(y ~ x + z, df)

# scatterplot
s3d <- scatterplot3d(x, z, y, pch = 19, type = "p", color = "darkgrey",
                     main = "Regression Plane", grid = TRUE, box = FALSE,  
                     mar = c(2.5, 2.5, 2, 1.5), angle = 30, zlab = "Test Score",
                     xlab = "Student-Teacher Ratio", ylab = "English")

# regression plane
s3d$plane3d(LM, draw_polygon = TRUE, draw_lines = TRUE, 
            polygon_args = list(col = rgb(.1, .2, .7, .5)))

# overlay positive residuals
wh <- resid(LM) > 0
s3d$points3d(x[wh], z[wh], y[wh], pch = 19)
```

We can see that the model appears to fit the data reasonably well, with many points being clustered around the regression plane. Naturally, we have some points that do not fit as well (remember we still have $u$ in the model!), as the added dimension of English speaking ability cannot explain away the unobserved error.

### Measures of Fit in the Multiple Linear Regression Model

Let's now call the full summary for our multiple regression model:

```{r}
summary(multi_mod)
```

We retain our previously covered measures of fit $R^2$ and standard error of regression $SER$ in multivariate regression, but now we are also interested in the adjusted-$R^2$, or $\bar{R}^2$

$R^2$ alone is no longer reliable for measuring goodness-of-fit for our model, because it's value increases with each right-side variable we add to the model. We could very easily game our analysis by adding many variables to our model, which decreases the $SSR$ and causes over-fitting in our model. $\bar{R}^2$ applies a penalty to each added variable $k$ by using a correction factor, calculated:

$$
\Large
\bar{R}^2 = 1 - \frac{n-1}{n-k-1}\frac{SSR}{TSS}
$$

The question remains - have we improved our model fit with the addition of another independent variable? We can compare the $\bar{R}^2$ from each model to find out!

```{r}
summary(linear_mod)$adj.r.squared
summary(multi_mod)$adj.r.squared
```

We can see a vast improvement in $\bar{R}^2$ between the two models!


#### Overfitting

So, why not just include as many variables as possible to account for everything in the world that may affect $test_scores$? That would lead to overfitting. The model will try to fit a line (or, in high-dimensional space where human brains begin to melt, a plane of some sort) that will try to capture all the variation in all observations. Although this may further reduce bias towards zero, meaning the model being very accurate at explaining that sample, the model will perform poorly with other datasets. This is because the 'variance' or variety of datasets the model has not seen before is large and different. This is what statisticians call the **variance-bias trade-off**. Again, remember that we used a data sample to infer and try to predict $test_scores$ based on information about schoolchildren in Californian schools. Having a very precise model here will not mean that this explains every other school in California, or even the world. This is why it is important to increase the explanatory power of the model by increasing the number of useful (not irrelevant) predictors in the model, but also avoiding adding a large amount of predictors that the model is highly biased towards the data it was trained on and fails to give insight on unseen data.




# 2. RCT

## Motivation

A randomised-controlled trial is characterised by random assignment of units (individuals, firms, etc.) to treatment and control groups (sometimes, even more).

Idea: assigning subjects randomly to treatment and control groups, they will be comparable on average. In other words, same characteristics on both sides (e.g., proportion of males to females, age distribution, etc.), so that any other factors that may influence X and y is roughly the same on both sides.

$$\Large \text{Sample size} \uparrow \longrightarrow \text{robustness} \uparrow$$


RCTs, in terms of statistical methods and interpretation of results, are simpler to understand than a simple linear regression (which is the bread and butter of any econometrician). The main focus is data and experiment!

For a treatment $D_i$ (1 = received treatment / 0 = not received treatment), we study outcome $Y_i$ of subject, namely $Y_{0i}$ if not received treatment and $Y_{1i}$ if received treatment. To know if the experiment (treatment) had an impact on the subjects, we want to know the **average treatment effect (ATE)**:

$$\Large E[Y_{1i} - Y_{0i}]$$
, or **average effect of treatment on the treated (ATT)**:

$$\Large E[Y_{1i} - Y_{0i} | D_i = 1]$$

In reality, when we compare the outcome across treatment and control, we have:

$$\Large E[Y_{1i} | D_i = 1] - E[Y_{0i} | D_i = 0] = E[Y_{1i} | D_i = 1] - E[Y_{0i} | D_i = 1] + E[Y_{0i} | D_i = 1] - E[Y_{0i} | D_i = 0]$$

The first two terms ATT, basically capturing the impact of treatment on the treated and not treated - this is what we want to estimate. Third and fourth terms capture the difference in outcome across the two groups in the hypothetical situation that nobody receives treatment. This is a *selection bias* (e.g., people that were already sick were going to the hospital).

By randomisation, if sample size is large, groups will be comparable before treatment, so that their expected outcome before treatment is the same, 

$$\Large E[Y_{0i} | D_i = 1] = E[Y_{0i} | D_i = 0]$$

and will only change when one gets treated and one does not get treated. This removes selection bias (groups will be comparable). The difference in outcomes after treatment will then be the impact of the treatment. 

What about the cons of RCTs?
- If not balanced, biased results!
- contamination and changed behaviour (*Hawthorne* and *John Henry* effects, for treatment and control groups, respectively)
- Attrition (discontinuity of data)



## Cognitive ability data

Let's look at a dataset that shows an experiment that aimed at increasing cognitive abilities of participants. We will look at participants that are allocated to a control and treatment group, at three different time periods.

Let's assume that we ran an intervention to improve walking fitness (i.e., 6MWT or "Meters Walked"). 
A secondary outcome would be cognitive performance measured using the ADAS-Cog (higher score => worse cognition).

There were three time points: *Baseline (Time=1)*, *Midpoint (Time=2; 3 months)*, and *Final (Time=3; 6 months)*.
The primary endpoint of the intervention is 6 months.INT= experimental group; CON= control group.

### Data processing

```{r}
# clean environment
rm( list = ls(all.names = TRUE) )

# wd
working_directory <- dirname(rstudioapi::getActiveDocumentContext()$path)
```


```{r}
# Loading dataset
cog_data <- read_excel( paste0( working_directory, "/rct_data.xlsx"))
```

```{r}
cat("Summary of variables:\n")
summary(cog_data)
cat("\nColumn names:")
colnames(cog_data)
```

Renaming vars

```{r}
cog_data <- cog_data %>%
  rename("Height" = "Height (cm)",
         "Weight" = "Weight (kg)",
         "BMI" = "BMI (kg/m2)",
         "Meters_Walked" = "6MWT",
         "ADAS_COG" = "ADAS-Cog Total")
```

Changing values

```{r}
cog_data$Education[cog_data$Education==3] <- "High School or Less"
cog_data$Education[cog_data$Education==2] <- "Trade School"
cog_data$Education[cog_data$Education==1] <- "University"

cog_data$Sex[cog_data$Sex==1] <- "Male"
cog_data$Sex[cog_data$Sex==0] <- "Female"

```

First, we need to set up the data for ANCOVA. This means we need our data to be switched from a long format to a wide format. Currently, dataset has id ($ID$), then time variable ($Time$), so we will switch it to wide so that there are several columns for the time-varying variables instead of storing everything in one column (this is what we mean by transforming data from long to wide).

In Stata, this can be done using the *reshape* command, in Python using *.pivot* or *pivot_table* 

```{r}
# Creating a data subset of data for timevarying variables

varying <- cog_data %>%
  select("ID", "Time", "ADAS_COG", "Meters_Walked")

# Creating a data subset for variables which do not vary over time
baseline <- cog_data %>%
  select(-"ADAS_COG", -"Meters_Walked") %>% # excluding
  filter(Time == 1)

# Creating data subsets for variables which vary over time at each timepoint
varying_1 <- subset(varying,Time==1)
varying_2 <- subset(varying,Time==2)
varying_3 <- subset(varying,Time==3)

# Label time varying variables at each time point (e.g., ADAS-Cog.1, ADAS-Cog.2, ADAS-Cog.3...)
colnames(varying_1) <- paste(colnames(varying_1),"1",sep="_")
colnames(varying_2) <- paste(colnames(varying_2),"2",sep="_")
colnames(varying_3) <- paste(colnames(varying_3),"3",sep="_")
```

When we convert data from long to wide, we can have a better view of each participant ($ID$) across time. 

```{r}
# Converting to wide (joining data by same ID)
wide_data <- left_join(baseline, varying_1, by=c("ID"="ID_1")) %>%   # joining datasets: baseline and varying_1
  left_join(.,varying_2,by=c("ID"="ID_2")) %>%                       # joining datasets: merged (baseline and varying_1) and varying_2
  left_join(.,varying_3,by=c("ID"="ID_3"))                           # joining datasets: merged (baseline, varying_1, varying_2) and varying_3
```

We see some attrition issues (missing data for $ADAS\_COG$ and $Meters\_Walked$) ...

```{r}
print(wide_data[wide_data$ID == "T_025", ])
```

Dropping Time variables (e.g., Time_1, Time_2, Time_3)

```{r}
# You can manually write them, or do this...
Time_vars <- grep("Time",colnames(wide_data),value=TRUE)
wide_data <- wide_data %>%
  select(-all_of(Time_vars)) # new versions of the library recommend you use of 'all_of'

```


### EDA

Now, we can look at the effects of this intervention to improve walking fitness. As a reminder, our primary endpoint is 6 months ($Meters\_Walked\_3$) at $Time == 3$. We are thus going to look at the effects of the intervention on walking fitness following 6 months training, while controlling for baseline walking fitness.

#### Descriptive stats

Let's see how the groups look like at baseline.

```{r}
cog_data_t1 <- cog_data %>%
  filter(Time == 1)

#For simplicity, our baseline descriptors which we're interested in are Age, BMI, Sex, Education, ADAS_Cog, and Meters_Walked
entity_vars <- cog_data_t1[, c("Age", "BMI", "Sex", "Education", "ADAS_COG", "Meters_Walked")] # filter data
entity_vars <- colnames(entity_vars) # get only column names

# Instead of fitlering the data for Group == CON and Group == INT, and then running summary stats for those, you can do:
summary_entity <- CreateTableOne(vars = entity_vars,  # variables for table
                                  strata = "Group",    # groupby 
                                  data = cog_data_t1
                                  )

print(summary_entity, contDigits=2, missing=TRUE, quote=TRUE)

```

At baseline (Time == 1), we have a few differences (not a lot though). For RCT, these differences (for variables of interest) don't matter as long as allocation of people in groups was (properly) done randomly. Although there are differences at baseline for $Meters\_Walked$, we will account for these differences when we run our regression using baseline variation.

We can show this difference visually by using a *for loop* that creates a bar chart for three different time periods, for each variable in entity_vars. The loop includes:

1. An if-else condition that separates numerical variables from categorical variables using "is.numeric"

2. Creates new datasets with mean values / count values using groupby

3. Plots the data using ggplot's *geom_bar* function

```{r}


for (var in entity_vars) {
  # Check if the variable is numeric
  if (is.numeric(cog_data[[var]])) {
    # Numerical variable: Calculate mean
    mean_data <- cog_data %>%
      group_by(Time, Group) %>%
      summarise(
        Mean_Value = mean(.data[[var]], na.rm = TRUE),  # Mean for numerical variables
        .groups = "drop"
      )
    # Create the barplot for mean
    plot <- ggplot(mean_data, aes(x = factor(Time), y = Mean_Value, fill = Group)) +
      geom_bar(stat = "identity", position = "dodge") +
      labs(
        title = paste("Average", var, "over Time by Group"),
        x = "Time",
        y = paste("Mean", var),
        fill = "Group"
      ) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
  } else {
    # Categorical variable: Count occurrences
    count_data <- cog_data %>%
      group_by(Time, Group, .data[[var]]) %>%
      summarise(
        Count = n(),  # Count occurrences for categorical variables
        .groups = "drop"
      )
    # Create the barplot for counts
    plot <- ggplot(count_data, aes(x = factor(Time), y = Count, fill = interaction(Group, .data[[var]]))) +
      geom_bar(stat = "identity", position = "dodge") +
      labs(
        title = paste("Count of", var, "over Time by Group and", var),
        x = "Time",
        y = "Count",
        fill = paste("Group and", var)
      ) +
      scale_fill_brewer(palette = "Set2") +  # Use a color palette for differentiation
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  }
  
  # Print the plot
  print(plot)
}
```

### Results

ANCOVA ("Analysis of covariance") is a general linear model that we can use to evaluate whether the means of a dependent variable are equal across levels of one or more categorical independent variablesand across one or more continuous variables. In our case, we want to see cognitive ability changes across time for the two groups of the experiment.

#### Dependent variable: $Meters\_Walked$

- **Case 1:** Time == 1 (baseline) vs Time == 2 (3 months)

```{r}
# 1. Basic ANCOVA model (run multivariate regression)
model_1_Meters_Walked_2 <- lm(Meters_Walked_2 ~ Group + Meters_Walked_1, data = wide_data)
summary(model_1_Meters_Walked_2)

# 2. ANCOVA model with additional covariates
model_2_Meters_Walked_2 <- lm(Meters_Walked_2 ~ Group + Meters_Walked_1 + Age + Sex, data=wide_data)
summary(model_2_Meters_Walked_2)

# No impact of treatment on meters walked
```


```{r}
# Let's look at the differences using least-squared means
lsmeans(model_1_Meters_Walked_2, ~Group)
lsmeans(model_2_Meters_Walked_2, ~Group)

# Using contrast, we can compare if these differences in means are significant
contrast_1<-contrast(lsmeans(model_2_Meters_Walked_2, ~Group), "trt.vs.ctrl", adjust = "none") 
print(contrast_1)

# Notice that the p-value of 0.9894 is the p-value from the regression results above !
# 
```

There is no significant difference in the amount of meters walked observed three months after the treatment.

- **Case 2:** Time == 1 (Baseline) vs Time == 3 (6 months)

```{r}
# 1. Basic ANCOVA model (run multivariate regression)
model_1_Meters_Walked_3 <- lm(Meters_Walked_3 ~ Group + Meters_Walked_1, data = wide_data)
summary(model_1_Meters_Walked_3)

# 2. ANCOVA model with additional covariates
model_2_Meters_Walked_3 <- lm(Meters_Walked_3 ~ Group + Meters_Walked_1 + Age + Sex, data=wide_data)
summary(model_2_Meters_Walked_3)

# No impact of treatment on meters walked
```

```{r}
# Let's look at the differences using least-squared means
lsmeans(model_1_Meters_Walked_3, ~Group)
lsmeans(model_2_Meters_Walked_3, ~Group)

# Using contrast, we can compare if these differences in means are significant
contrast_1<-contrast(lsmeans(model_2_Meters_Walked_3, ~Group), "trt.vs.ctrl", adjust = "none") 
print(contrast_1)
```

There is no significant difference in the amount of meters walked observed six months after the treatment.


#### Dependent variable: $ADAS\_COG$

- **Case 1:** Time == 1 (baseline) vs Time == 2 (3 months)

```{r}
# 1. Basic ANCOVA model (run multivariate regression)
model_1_ADAS_COG_2 <- lm(ADAS_COG_2 ~ Group + ADAS_COG_1, data = wide_data)
summary(model_1_ADAS_COG_2)

# 2. ANCOVA model with additional covariates
model_2_ADAS_COG_2 <- lm(ADAS_COG_2 ~ Group + ADAS_COG_1 + Age + Sex + Education, data=wide_data)
summary(model_2_ADAS_COG_2)

# Size of impact (coefficient increases), sign of OVB, but still not signficant => No impact of treatment on cognitive ability
```

```{r}
lsmeans(model_1_ADAS_COG_2, ~Group)
lsmeans(model_2_ADAS_COG_2, ~Group)

# Using contrast, we can compare if these differences in means are significant
contrast_1<-contrast(lsmeans(model_2_ADAS_COG_2, ~Group), "trt.vs.ctrl", adjust = "none") 
print(contrast_1)
```

- **Case 2:** Time == 1 (Baseline) vs Time == 3 (6 months)

```{r}
# 1. Basic ANCOVA model (run multivariate regression)
model_1_ADAS_COG_3 <- lm(ADAS_COG_3 ~ Group + ADAS_COG_1, data = wide_data)
summary(model_1_ADAS_COG_3)

# 2. ANCOVA model with additional covariates
model_2_ADAS_COG_3 <- lm(ADAS_COG_3 ~ Group + ADAS_COG_1 + Age + Sex + Education, data=wide_data)
summary(model_2_ADAS_COG_3)

# Size of impact (coefficient increases), sign of OVB, but still not signficant => No impact of treatment on cognitive ability
```

```{r}
lsmeans(model_1_ADAS_COG_3, ~Group)
lsmeans(model_2_ADAS_COG_3, ~Group)

# Using contrast, we can compare if these differences in means are significant
contrast_1<-contrast(lsmeans(model_2_ADAS_COG_3, ~Group), "trt.vs.ctrl", adjust = "none") 
print(contrast_1)
```

No significant impact on cognitive ability 6 months after treatment either!


**IMPORTANT NOTE:** if you follow tutorial at https://www.youtube.com/watch?v=Mg9eneLECDg, you will see different results for ADAS_COG (significant impact). For some reason, the raw data Ryan used has different values for this variable in the video, compared to the raw data that is available in the Git repository.

